{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "educational-chosen",
   "metadata": {},
   "source": [
    "# Using EcoFOCIpy to process raw field data\n",
    "\n",
    "## Mooring / Timeseries Data\n",
    "\n",
    "Basic workflow for each instrument grouping is as follows:\n",
    "- Parse data from raw files into pandas dataframe\n",
    "- output initial files (pandas->csv) **ERDDAP NRT** when no meta data is added\n",
    "\n",
    "Convert to xarray dataframe for all following work *(working or final data level):*\n",
    "- Add metadata from instrument yaml files and/or header info\n",
    "- ingest metadata from deployment/recovery records or cast logs\n",
    "- process data beyond simple file translate\n",
    "- apply any calibrations or corrections\n",
    "    + field corrections\n",
    "    + offsets\n",
    "    + instrument compensations\n",
    "    + some QC were available... this would be old-school simple bounds mostly\n",
    "- adjust time bounds and sample frequency (xarray dataframe)\n",
    "- save as CF netcdf via xarray: so many of the steps above are optional\n",
    "    + **ERDDAP/CF NRT** often if no corrections, offsets or time bounds are applied. Occasionally some meta data is\n",
    "    + **Working and awaiting QC** has no ERDDAP representation and is a holding spot\n",
    "    + **ERDDAP/CF Final** fully calibrated, qc'd and populated with meta information\n",
    "\n",
    "Plot for preview and QC\n",
    "- preview images (indiv and/or collectively)\n",
    "- manual qc process\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-miniature",
   "metadata": {},
   "source": [
    "## Example below is for 4-beam ADCP but the workflow is similar for all ADCP's with this output format.\n",
    "\n",
    "Future processing of this instrument can be a simplified (no markdown) process which can be archived so that the procedure can be traced or updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "studied-pollution",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import EcoFOCIpy.io.adcp_parser as adcp_parser #<- instrument specific\n",
    "import EcoFOCIpy.io.ncCFsave as ncCFsave\n",
    "import EcoFOCIpy.metaconfig.load_config as load_config\n",
    "import EcoFOCIpy.math.geotools as geotools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-millennium",
   "metadata": {},
   "source": [
    "The sample_data_dir should be included in the github package but may not be included in the pip install of the package\n",
    "\n",
    "## Simple Processing - first step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d7f1f01-6cee-4e6f-b9c8-b2caa72c73ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data_dir = '/Users/bell/Programs/EcoFOCIpy/'\n",
    "user_data_dir = '/Users/bell/ecoraid/2023/Moorings/23bsp5a/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "third-yellow",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# edit to point to {instrument sepcific} raw datafile \n",
    "datafile = user_data_dir+'rawconverted/adcp/'\n",
    "instrument = 'RDI 300 KHz ADCP 1705'\n",
    "serial = '1705'\n",
    "mooring_meta_file = user_data_dir+'logs/23BSP-5A.yaml'\n",
    "inst_meta_file = sample_data_dir+'staticdata/instr_metaconfig/adcp.yaml'\n",
    "inst_shortname = 'wcp'\n",
    "###############################################################\n",
    "\n",
    "#init and load data : there are usually 5 files associated with ADCP data\n",
    "adcp_wop = adcp_parser.adcp(serialno=serial,depdir=datafile)\n",
    "adcp_pg_data = adcp_wop.load_pg_file() \n",
    "adcp_ein_data = adcp_wop.load_ein_file() \n",
    "adcp_scal_data = adcp_wop.load_scal_file() \n",
    "adcp_vel_data = adcp_wop.load_vel_file() \n",
    "\n",
    "#load report file\n",
    "adcp_rpt = adcp_wop.load_rpt_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-military",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_scal_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2568efb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_pg_data.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b886190",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_rpt[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-active",
   "metadata": {},
   "source": [
    "## Add Deployment meta information\n",
    "\n",
    "Load for future use, but in case of ADCP early loading allows declination correction based on recorded location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-fairy",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#just a dictionary of dictionaries - simple\n",
    "with open(mooring_meta_file) as file:\n",
    "    mooring_config = yaml.full_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-begin",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mooring_config['Instrumentation'][instrument]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742cc40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Deployment Lat: {mooring_config['Deployment']['DeploymentLatitude']}\")\n",
    "print(f\"Deployment Lon: {mooring_config['Deployment']['DeploymentLongitude']}\")\n",
    "print(f\"Deployment Date: {mooring_config['Deployment']['DeploymentDateTimeGMT'].date()}\")\n",
    "\n",
    "#some parsing/cleaning for functions\n",
    "latlon_dec = geotools.latlon_convert(mooring_config['Deployment']['DeploymentLatitude'],\n",
    "                        mooring_config['Deployment']['DeploymentLongitude'])\n",
    "dep_date = mooring_config['Deployment']['DeploymentDateTimeGMT'].date()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b105a3-cfca-455c-bac5-85498b75c506",
   "metadata": {},
   "source": [
    "## Do we know the actual depth yet? or just the designed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be902905-f4fd-4f9f-b3b2-d31b2cec459d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DepthDescription = 'ActualDepth' #'ActualDepth' or 'DesignedDepth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4824c7",
   "metadata": {},
   "source": [
    "## ADCP Internal processing\n",
    "\n",
    "+ Use the PG file to filter the velocities\n",
    "+ apply magnetic declination correction based on location and deployment date\n",
    "+ Calculate bin depths using RPT information (and validate against deployment info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f27a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#build array of indexes where pg is less then 25 and mask out velocity data\n",
    "adcp_vel_data[adcp_pg_data['pg4beam-good'] < 25][['u_curr_comp','v_curr_comp','w_curr_comp','w_curr_comp_err']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ecef1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_wop.mag_dec_corr(latlon_dec[0],latlon_dec[1],dep_date,apply_correction=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3826f0-6e9a-43ac-bcf8-45754f6ac9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#watch for units... cm vs m\n",
    "depths = (adcp_wop.bins2depth(depth_int=adcp_rpt[1]['bin_length'],\n",
    "           depth2firstbin=adcp_rpt[1]['distance'],\n",
    "           numofbins=adcp_rpt[1]['numofbins'],\n",
    "           inst_depth=mooring_config['Instrumentation'][instrument][DepthDescription]*100.) / 100.).round()\n",
    "\n",
    "d = np.array([])\n",
    "for i in range(0,int(adcp_vel_data.shape[0] / len(depths))):\n",
    "        d = np.hstack((d,depths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2748fd9-57f6-4365-a8a2-4e343ea819d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#unit convert to cm/s from mm/s\n",
    "adcp_vel_data[['u_curr_comp','v_curr_comp','w_curr_comp','w_curr_comp_err']] = adcp_vel_data[['u_curr_comp','v_curr_comp','w_curr_comp','w_curr_comp_err']] / 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383c89d7-ea31-4697-8a7e-3ff6259455fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_vel_data[adcp_pg_data['pg4beam-good'] < 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d88bae-985d-4a67-b87b-0c0782b274af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change in processing - merge vel and ein data into a single file\n",
    "adcp_vel_data[adcp_pg_data['pg4beam-good'] < 25][['u_curr_comp','v_curr_comp','w_curr_comp','w_curr_comp_err']] = np.nan\n",
    "\n",
    "#mask known missing values (9999.9)\n",
    "adcp_vel_data = adcp_vel_data.where(adcp_vel_data != 9999.9)\n",
    "\n",
    "adcp_velein_data = pd.concat([adcp_vel_data.reset_index().set_index(['date_time','bin']),adcp_ein_data.reset_index().set_index(['date_time','bin'])],axis=1)\n",
    "\n",
    "#replace bins with depths\n",
    "# change reversed from True to False if necessary based on EIN plots\n",
    "reversed = False\n",
    "if reversed:\n",
    "    adcp_velein_data['depth'] = d\n",
    "else:\n",
    "    adcp_velein_data['depth'] = d[::-1]\n",
    "\n",
    "adcp_velein_data = adcp_velein_data.reset_index().drop('bin',axis=1).set_index(['date_time','depth'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-audit",
   "metadata": {},
   "source": [
    "#### Time properties\n",
    "\n",
    "Its unusual that our clocks drift to the point of concern for our instruments (if an instrument is off by 3 minutes but only sampling hourly... regridding that data will result in minimal changes).  However, there are a few time oriented modifications that may need to be made.\n",
    "\n",
    "The can be classified into two categories:\n",
    "+ interpolate: these change the parameter values in accordance with the time edits\n",
    "    - linear interpolation is most common\n",
    "    - averaging of data and rebinning/resampling is also common (this needs to have the \"time lable\" thought out...)\n",
    "    - decimating is less common but does not impact the max/min values\n",
    "+ shift: these do not alter the measurements, just the timestamps they are associated with\n",
    "    - the round function will work well to correct small time errors/drifts **common**\n",
    "    - dropping extra precision on time (if you want hourly measurements, just remove all minute/second info... could cause large errors if rounding would have been more appropriate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supposed-bankruptcy",
   "metadata": {},
   "source": [
    "It is very easy to use pandas interplation and resample methods on the dataframe as is.  A few steps are suggested below:\n",
    "- parse out on-deck (predeployment and recovery) data.  This can be done via pandas or xarray but requires the mooring metadata to have been read in.  See future steps below.\n",
    "- even if the sample frequency is set to the desired measurement frequency, it would be good to perform a quick regridding as an assurance task\n",
    "- FOCI data is usualy 1min, 10min, 1hr - and the 1min data is a fairly new (sbe56) data stream\n",
    "    + subsampling high frequency data to lower frequency is easy via df.resample().mean() but it will label the new datapoint per default instructions.  The default is to label it with the left boundary of the bin.\n",
    "    + you may want to take the median instead of the mean for noisy data (fluorometer) , occasionally decimating may be more appropriate if you want to downsize the dataset size but not smear features\n",
    "    + shifting times can be a bit more involved.  There are two primary ways to do it, interpolate or shift (round)\n",
    "        - to interpolate, you will need to upsample your data to a higher frequency which will generate missing values, then interpolate (with a maximum gap size), then decimate.  This always has the artifact of smoothing data and decreasing the min/max values. **common on microcats and other 10min datasets**\n",
    "        - shifting usually just involves droping extra time \"digits\", if you want hourly, you could just drop the trailing minutes assuming you are just off the hour (8:05 -> 8:00) or you can round to the nearest time unit but niether of these changes the data value, just the time associated with it. **common on seacats and other hourly datasets**\n",
    "        - you may also be able to *shift* using the pandas datetime round function and specifing the desired frequency.\n",
    "    + I suggest if no change is needed... df.index.round(freq=*'your native sample freq'*)\n",
    "    \n",
    "*NOTE:* ADCP is 2D so be sure to groupby bins when using time/freq methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f031f85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_scal_data.index = adcp_scal_data.index.round(freq='1h') #round the time, no interpolation\n",
    "adcp_scal_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-membrane",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_scal_data.plot(figsize=(16,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d38f4a-7a9c-47df-b0d9-00454576dc61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=len(adcp_scal_data.columns), sharex=True, figsize=(16,8))\n",
    "\n",
    "for count,parameter in enumerate(adcp_scal_data.columns):\n",
    "    adcp_scal_data[parameter].plot(ax=ax[count],label=parameter)\n",
    "    ax[count].legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-cornwall",
   "metadata": {},
   "source": [
    "## Add Instrument meta information\n",
    "\n",
    "Time, depth, lat, lon should be added regardless (always our coordinates) but for a mooring site its going to be a (1,1,1,t) dataset\n",
    "The variables of interest should be read from the data file and matched to a key for naming.  That key is in the inst_config file seen below and should represent common conversion names in the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-raise",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(inst_meta_file) as file:\n",
    "    inst_config = yaml.full_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-popularity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add meta data and prelim processing based on meta data\n",
    "# Convert to xarray and add meta information - save as CF netcdf file\n",
    "# pass -> data, instmeta, depmeta\n",
    "## Note: EIN and VEL are 2D while scal is 1D.  Combine EIN and VEL into a single file? (more may use the backscatter data)\n",
    "##  but the scalar data can be treated like any other instrument and will be maintained in a separate file\n",
    "adcp_wop_nc = ncCFsave.EcoFOCI_CFnc(df=adcp_scal_data, \n",
    "                                instrument_yaml=inst_config, \n",
    "                                operation_yaml=mooring_config,\n",
    "                                operation_type='mooring', \n",
    "                                instrument_id=instrument, \n",
    "                                inst_shortname='scal')\n",
    "adcp_wop_nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-chambers",
   "metadata": {},
   "source": [
    "At this point, you could save your file with the `.xarray2netcdf_save()` method and have a functioning dataset.... but it would be very simple with no additional qc, meta-data, or tuned parameters for optimizing software like ferret or erddap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-pennsylvania",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# expand the dimensions and coordinate variables\n",
    "# renames them appropriatley and prepares them for meta-filled values\n",
    "adcp_wop_nc.expand_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-operations",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_wop_nc.variable_meta_data(variable_keys=list(adcp_wop.load_scal_file().columns.values),drop_missing=True)\n",
    "adcp_wop_nc.temporal_geospatioal_meta_data(depth='actual')\n",
    "#adding dimension meta needs to come after updating the dimension values... BUG?\n",
    "adcp_wop_nc.dimension_meta_data(variable_keys=['depth','latitude','longitude'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-product",
   "metadata": {},
   "source": [
    "The following steps can happen in just about any order and are all meta-data driven.  Therefore, they are not required to have a functioning dataset, but they are required to have a well described dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-diversity",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#add global attributes\n",
    "adcp_wop_nc.deployment_meta_add()\n",
    "adcp_wop_nc.get_xdf()\n",
    "\n",
    "#add instituitonal global attributes\n",
    "adcp_wop_nc.institution_meta_add()\n",
    "\n",
    "#add creation date/time - provenance data\n",
    "adcp_wop_nc.provinance_meta_add()\n",
    "\n",
    "#provide intial qc status field\n",
    "adcp_wop_nc.qc_status(qc_status='unknown')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0387b93c-e7ee-4f32-9dd6-e54c10e81763",
   "metadata": {},
   "source": [
    "### Plot Trimmed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ba8005-ce3b-441a-a7d6-6b009306894c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_wop_nc.autotrim_time().items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a198c4-bab5-48f1-b750-95a2c53112ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=len(adcp_scal_data.columns), sharex=True, figsize=(16,8))\n",
    "\n",
    "count=0\n",
    "for parameter,values in adcp_wop_nc.autotrim_time().items():\n",
    "    values.plot(ax=ax[count],label=parameter)\n",
    "    ax[count].legend(loc='upper left')\n",
    "    ax[count].set_ylabel('')\n",
    "    count+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-hughes",
   "metadata": {},
   "source": [
    "## Save CF Netcdf files\n",
    "\n",
    "Currently stick to netcdf3 classic... but migrating to netcdf4 (default) may be no problems for most modern purposes.  Its easy enough to pass the `format` kwargs through to the netcdf api of xarray.\n",
    "\n",
    "An untrimmed copy should go in initial_archive (has deck data and is the first simple round of pre-QC)\n",
    "A trimmed, filtered, filled copy with QA/QC will be derived after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-volunteer",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# combine trim (not mandatory) and filename together (saves to test.nc without name)\n",
    "#adcp_wop_nc.xarray2netcdf_save(xdf = adcp_wop_nc.autotrim_time(),filename='test.nc',format=\"NETCDF3_CLASSIC\")\n",
    "\n",
    "# don't trim the data and pass your own filename\n",
    "inst_shortname = 'scal'\n",
    "depth = str(int(mooring_config['Instrumentation'][instrument][DepthDescription])).zfill(4)\n",
    "filename = \"\".join(mooring_config['MooringID'].split('-')).lower()+'_'+inst_shortname+'_'+depth+'m.nc'\n",
    "adcp_wop_nc.xarray2netcdf_save(xdf = adcp_wop_nc.get_xdf(),filename=filename,format=\"NETCDF4_CLASSIC\") #<- if you  would rather keep untrimmed time data\n",
    "# adcp_wop_nc.xarray2netcdf_save(xdf = adcp_wop_nc.autotrim_time(),filename=filename,format=\"NETCDF4_CLASSIC\") #<-- to save a trimmed copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-breathing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_wop_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4d536-7bb1-4b7f-aead-646fff97cce0",
   "metadata": {},
   "source": [
    "## Do same as above with 2D (EIN & VEL data) - skip most descriptions though\n",
    "\n",
    "- Historically, FOCI/EPIC standards have kept EIN and Vel data in seperate files.  We will now maintain a single file, but will need to answer the question of QC'ing EIN data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afaa7fb-f0d1-436c-8fc4-42f7607d8c3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#workds with 2d files same as 1d... just pass a multi-index dataframe in to method\n",
    "adcp_2D_nc = ncCFsave.EcoFOCI_CFnc(df=adcp_velein_data, \n",
    "                                instrument_yaml=inst_config, \n",
    "                                operation_yaml=mooring_config,\n",
    "                                operation_type='mooring', \n",
    "                                instrument_id=instrument, \n",
    "                                inst_shortname='wcp')\n",
    "\n",
    "adcp_2D_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736d3ba-4aa3-4dac-8125-77f61ae0e785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optional plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(24,8))\n",
    "try:\n",
    "    adcp_2D_nc.get_xdf()['agc1'].T.plot(ax=axes[0,0], yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['agc2'].T.plot(ax=axes[0,1], yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['agc3'].T.plot(ax=axes[1,0], yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['agc4'].T.plot(ax=axes[1,1], yincrease=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(24,8))\n",
    "try:\n",
    "    adcp_2D_nc.get_xdf()['u_curr_comp'].T.plot(ax=axes[0,0],vmin=0, vmax=500, yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['v_curr_comp'].T.plot(ax=axes[0,1],vmin=0, vmax=500, yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['w_curr_comp'].T.plot(ax=axes[1,0],vmin=0, vmax=500, yincrease=False)\n",
    "    adcp_2D_nc.get_xdf()['w_curr_comp_err'].T.plot(ax=axes[1,1],vmin=0, vmax=500, yincrease=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17cc26-452e-459a-84d5-f32c71653f68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_2D_nc.get_xdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67af39eb-1a3d-4f7e-a6cf-b32cd8bf0984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_2D_nc.expand_dimensions(dim_names=['latitude','longitude'],geophys_sort=False)\n",
    "adcp_2D_nc.variable_meta_data(variable_keys=list(adcp_velein_data.columns.values),drop_missing=True)\n",
    "adcp_2D_nc.temporal_geospatioal_meta_data(depth='adcp') #passing adcp skips adding record data to adcp 2d variable\n",
    "#adding dimension meta needs to come after updating the dimension values... BUG?\n",
    "adcp_2D_nc.dimension_meta_data(variable_keys=['depth','latitude','longitude'])\n",
    "\n",
    "#add global attributes\n",
    "adcp_2D_nc.deployment_meta_add()\n",
    "\n",
    "#add instituitonal global attributes\n",
    "adcp_2D_nc.institution_meta_add()\n",
    "\n",
    "#add creation date/time - provenance data\n",
    "adcp_2D_nc.provinance_meta_add()\n",
    "\n",
    "#provide intial qc status field\n",
    "adcp_2D_nc.qc_status(qc_status='unknown')\n",
    "\n",
    "inst_shortname = 'wcp'\n",
    "depth = str(int(mooring_config['Instrumentation'][instrument][DepthDescription])).zfill(4)\n",
    "filename = \"\".join(mooring_config['MooringID'].split('-')).lower()+'_'+inst_shortname+'_'+depth+'m.nc'\n",
    "adcp_2D_nc.xarray2netcdf_save(xdf = adcp_2D_nc.get_xdf(),filename=filename,format=\"NETCDF4_CLASSIC\") #<- if you  would rather keep untrimmed time data\n",
    "adcp_2D_nc.xarray2netcdf_save(xdf = adcp_2D_nc.autotrim_time(),filename=filename.replace('.nc','.trimmed.nc'),format=\"NETCDF4_CLASSIC\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b75de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adcp_2D_nc.autotrim_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c61da8-adeb-402b-8053-7772c8f36b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optional plotting\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(24,8))\n",
    "try:\n",
    "    adcp_2D_nc.autotrim_time()['agc1'].T.plot(ax=axes[0,0], yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['agc2'].T.plot(ax=axes[0,1], yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['agc3'].T.plot(ax=axes[1,0], yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['agc4'].T.plot(ax=axes[1,1], yincrease=False)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, figsize=(24,8))\n",
    "try:\n",
    "    adcp_2D_nc.autotrim_time()['u_curr_comp'].T.plot(ax=axes[0,0],vmin=0, vmax=200, yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['v_curr_comp'].T.plot(ax=axes[0,1],vmin=0, vmax=200, yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['w_curr_comp'].T.plot(ax=axes[1,0],vmin=0, vmax=200, yincrease=False)\n",
    "    adcp_2D_nc.autotrim_time()['w_curr_comp_err'].T.plot(ax=axes[1,1],vmin=0, vmax=200, yincrease=False)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-nature",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "QC of data (plot parameters with other instruments)\n",
    "- be sure to updated the qc_status and the history\n",
    "\n",
    "- see [EcoFOCIpy_2D_filter_example.ipynb](EcoFOCIpy_2D_filter_example.ipynb) for how to apply a lanzcos filter to each depth and return usable results\n",
    "- see [EcoFOCIpy_ADCP_QC_example.ipynb](EcoFOCIpy_ADCP_QC_example.ipynb) for how to apply basic ADCP QC (beyond adjusting the depth and trimming times which you've already done).  The additional QC may involve removing bins above surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3504b0e-7820-46f2-8df2-43d721c5068b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p312]",
   "language": "python",
   "name": "conda-env-p312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
